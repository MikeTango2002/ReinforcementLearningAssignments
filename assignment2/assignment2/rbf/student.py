import random
import numpy as np
# import gymnasium as gym
# import time
# from gymnasium import spaces
# import os
import sklearn
import sklearn.pipeline
import sklearn.preprocessing
from sklearn.kernel_approximation import RBFSampler
from sklearn.preprocessing import StandardScaler
import pickle


class VanillaFeatureEncoder:
    def __init__(self, env):
        self.env = env
        
    def encode(self, state):
        return state
    
    @property
    def size(self): 
        return self.env.observation_space.shape[0]

class RBFFeatureEncoder:
    def __init__(self, env): # modify
        self.env = env
        # TODO init rbf encoder
        
        # Fit the RBF sampler to some sample data from the environment
        # MountainCar state has 2 dimensions (position, velocity)
        # Samples random projection according to n_features.
        # It needs as input some data to compute the mean and variance of each feature.
        # The shape of the input should be (n_samples, n_features)
        # Each sample has shape (2,) for MountainCar, and np.array(list_of_arrays) automatically stacks them correctly
        observation_examples = np.array([env.observation_space.sample() for i in range(5000)])

        self.scaler = StandardScaler()

        self.scaler.fit(observation_examples)

        scaled_examples = self.scaler.transform(observation_examples)

        ## OLD APPROACH ##
        ##################
        # n_components is the number of features to generate
        #self.rbf_sampler = RBFSampler(n_components=300, gamma=0.9, random_state=1)
        #self.rbf_sampler.fit(scaled_examples)
        ##################

        self.samplers = []
        gammas = [0.2, 0.5, 1.0, 2.0] 
        n_components_per_gamma = 500
        
        for g in gammas:
            sampler = RBFSampler(n_components=n_components_per_gamma, gamma=g, random_state=1)
            sampler.fit(scaled_examples)
            self.samplers.append(sampler)

        self._size = len(gammas) * n_components_per_gamma

        

    def encode(self, state): # modify
        # TODO use the rbf encoder to return the features

        # the transform method takes x as a 2D array (shape: [n_samples, n_features]) 
        # where n_samples is the number of samples to transform (1 in our case)
        # and n_features is the number of features in the input state (2 for MountainCar).
        # The features of the input state are different from the features generated by the RBF sampler.
        # Since the observation space in MountainCar is composed of ndarrays of shape (2,), we need to reshape the state to a 2D array with shape (1, 2)
        state_2d = np.array(state).reshape(1, -1) 

        scaled_state = self.scaler.transform(state_2d)

        # Transform using all samplers and concatenate
        features_list = [sampler.transform(scaled_state) for sampler in self.samplers]

        # Concatenate along axis 1 (columns) and flatten
        combined_features = np.concatenate(features_list, axis=1)
        
        return combined_features.flatten()

    @property
    def size(self): # modify
        # TODO return the number of features
        return self._size 

class TDLambda_LVFA:
    def __init__(self, env, feature_encoder_cls=RBFFeatureEncoder, alpha=0.01, alpha_decay=1.0, 
                 gamma=0.9999, epsilon=0.3, epsilon_decay=0.995, final_epsilon=0.2, lambda_=0.9): # modify if you want (e.g. for forward view)
        self.env = env
        self.feature_encoder = feature_encoder_cls(env)
        self.shape = (self.env.action_space.n, self.feature_encoder.size)
        self.weights = np.random.random(self.shape)
        self.traces = np.zeros(self.shape)
        self.alpha = alpha
        self.alpha_decay = alpha_decay
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.final_epsilon = final_epsilon
        self.lambda_ = lambda_
        
    def Q(self, feats):
        feats = feats.reshape(-1,1)
        return self.weights@feats
    
    def update_transition(self, s, action, s_prime, reward, done): # modify
        s_feats = self.feature_encoder.encode(s)
        s_prime_feats = self.feature_encoder.encode(s_prime)

        # TODO update the weights

        #print(self.traces[action].shape)
        #print(s_feats.shape)


        next_q_values = self.Q(s_prime_feats)
        current_q = self.Q(s_feats)[action]

        # Calculate TD Error
        # Q-Learning Delta (TD Target - Current Q)
        target = reward + self.gamma * np.max(next_q_values) * (1 - done)
        delta = target - current_q

        # Update Eligibility Traces
        self.traces *= self.gamma * self.lambda_
        self.traces[action] += s_feats
   
        # Update Weights
        self.weights[action] += self.alpha * delta * self.traces[action]

        
        
    def update_alpha_epsilon(self): # do not touch
        self.epsilon = max(self.final_epsilon, self.epsilon*self.epsilon_decay)
        self.alpha = self.alpha*self.alpha_decay
        
    def policy(self, state): # do not touch
        state_feats = self.feature_encoder.encode(state)
        return self.Q(state_feats).argmax()
    
    def epsilon_greedy(self, state, epsilon=None): # do not touch
        if epsilon is None: epsilon = self.epsilon
        if random.random()<epsilon:
            return self.env.action_space.sample()
        return self.policy(state)
       
        
    def train(self, n_episodes=200, max_steps_per_episode=200): # do not touch
        print(f'ep | eval | epsilon | alpha')
        for episode in range(n_episodes):
            done = False
            s, _ = self.env.reset()
            self.traces = np.zeros(self.shape)
            for i in range(max_steps_per_episode):
                
                action = self.epsilon_greedy(s)
                s_prime, reward, done, _, _ = self.env.step(action)
                self.update_transition(s, action, s_prime, reward, done)
                
                s = s_prime
                
                if done: break
                
            self.update_alpha_epsilon()

            if episode % 20 == 0:
                print(episode, self.evaluate(), self.epsilon, self.alpha)
                
    def evaluate(self, env=None, n_episodes=10, max_steps_per_episode=200): # do not touch
        if env is None:
            env = self.env
            
        rewards = []
        for episode in range(n_episodes):
            total_reward = 0
            done = False
            s, _ = env.reset()
            for i in range(max_steps_per_episode):
                action = self.policy(s)
                
                s_prime, reward, done, _, _ = env.step(action)
                
                total_reward += reward
                s = s_prime
                if done: break
            
            rewards.append(total_reward)
            
        return np.mean(rewards)

    def save(self, fname):
        with open(fname, 'wb') as f:
            pickle.dump(self, f)

    @classmethod
    def load(cls, fname):
        return pickle.load(open(fname,'rb'))
